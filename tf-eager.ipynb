{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as eager\n",
    "import my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.depth = 10\n",
    "args.device = '/cpu:0'\n",
    "# args.device = '/device:GPU:0' # TODO\n",
    "args.graph = 'soc-Epinions1'\n",
    "# args.graph = 'soc-Slashdot0811'\n",
    "# args.graph = 'soc-Slashdot0902'\n",
    "args.lambda_node = 1\n",
    "args.lambda_edge = 1\n",
    "args.lambda_entropy = 1\n",
    "args.n_features = 16\n",
    "args.n_iterations = 500\n",
    "args.n_machines = 10\n",
    "args.radius = 3\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--depth', type=int, default=10)\n",
    "# parser.add_argument('--device', type=str, default='/cpu:0')\n",
    "# parser.add_argument('--graph', type=str, default='soc-Epinions1')\n",
    "# parser.add_argument('--lambda_node', type=float, default=1)\n",
    "# parser.add_argument('--lambda_edge', type=float, default=1)\n",
    "# parser.add_argument('--lambda_entropy', type=float, default=1)\n",
    "# parser.add_argument('--n-features', type=int, default=16)\n",
    "# parser.add_argument('--n-iterations', type=int, default=500)\n",
    "# parser.add_argument('--n-machines', type=int, default=10)\n",
    "# parser.add_argument('--radius', type=int, default=3)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "keys = sorted(vars(args).keys())\n",
    "run_id = '-'.join('%s-%s' % (key, str(getattr(args, key))) for key in keys if key != 'device')\n",
    "writer = tf.contrib.summary.create_file_writer('runs/' + run_id)\n",
    "writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    def __init__(self, adj, n_machines, lambda_node, lambda_edge, lambda_entropy):\n",
    "        adj = sp.sparse.triu(adj)\n",
    "        self.n_nodes, self.n_edges = adj.shape[0], len(adj.data)\n",
    "        adj = my.sparse_sp2tf(adj)\n",
    "        adj = tf.cast(adj, tf.float32)\n",
    "        e_idx = tf.expand_dims(tf.range(0, self.n_edges, dtype=tf.int64), 1)\n",
    "        idx0 = tf.concat((tf.expand_dims(adj.indices[:, 0], 1), e_idx), 1)\n",
    "        idx1 = tf.concat((tf.expand_dims(adj.indices[:, 1], 1), e_idx), 1)\n",
    "        idx = tf.concat((idx0, idx1), 0)\n",
    "        self.s = tf.SparseTensor(idx, tf.ones(idx.shape[0]), dense_shape=(self.n_nodes, self.n_edges))\n",
    "        self.n_machines = n_machines\n",
    "        self.lambda_node, self.lambda_edge, self.lambda_entropy = lambda_node, lambda_edge, lambda_entropy\n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        # TODO incorporate extent of concentration in objective\n",
    "        # TODO return expected number of edges per machine\n",
    "\n",
    "        p = tf.nn.softmax(x)\n",
    "        log_p = tf.nn.log_softmax(x)\n",
    "        entropy = -tf.reduce_sum(p * log_p) / float(self.n_edges)\n",
    "        \n",
    "        if training:\n",
    "            y = tf.multinomial(p, num_samples=1)\n",
    "            y = tf.squeeze(y)\n",
    "            y = tf.one_hot(y, self.n_machines)\n",
    "        else:\n",
    "            y = p\n",
    "        \n",
    "        z = tf.sparse_tensor_dense_matmul(self.s, y)\n",
    "        z = tf.minimum(z, 1)\n",
    "\n",
    "        sum_z = tf.reduce_sum(z)\n",
    "        r = sum_z / float(self.n_nodes)\n",
    "        \n",
    "        q_node = (tf.reduce_sum(z, 0) + 1) / sum_z\n",
    "        b_node = tf.reduce_sum(q_node * tf.log(q_node))\n",
    "        \n",
    "        q_edge = (tf.reduce_sum(y, 0) + 1) / tf.reduce_sum(y)\n",
    "        b_edge = tf.reduce_sum(q_edge * tf.log(q_node))\n",
    "\n",
    "        if training:\n",
    "#             joint = r\n",
    "            joint = r + self.lambda_node * b_node + self.lambda_edge * b_edge + self.lambda_entropy * entropy\n",
    "            objective = joint * tf.reduce_sum(y * log_p) / float(self.n_edges)\n",
    "            return objective, r, b_node, b_edge, entropy\n",
    "        else:\n",
    "            return r, b_node, b_edge, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModule(tf.keras.Model):\n",
    "    def __init__(self, in_features, out_features, adj, deg, radius, activation):\n",
    "        super().__init__()\n",
    "        self.adj, self.deg, self.radius = adj, deg, radius\n",
    "        new_dense = lambda: tf.keras.layers.Dense(input_shape=(in_features,), units=out_features, use_bias=False)\n",
    "        self.alpha1, self.alpha2, self.alpha3 = new_dense(), new_dense(), new_dense()\n",
    "        for i in range(radius):\n",
    "            setattr(self, 'alpha%d' % (i + 4), new_dense())\n",
    "        self.beta1, self.beta2, self.beta3 = new_dense(), new_dense(), new_dense()\n",
    "        for i in range(radius):\n",
    "            setattr(self, 'beta%d' % (i + 4), new_dense())\n",
    "        self.bn_alpha, self.bn_beta = tf.keras.layers.BatchNormalization(axis=1), tf.keras.layers.BatchNormalization(axis=1)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        deg = self.deg * x\n",
    "        u = tf.reduce_mean(x, 1, keepdims=True) + tf.zeros_like(x)\n",
    "        adj = [tf.sparse_tensor_dense_matmul(self.adj, x)]\n",
    "        matmul = lambda x, y: tf.sparse_tensor_dense_matmul(y, x)\n",
    "        for i in range(self.radius - 1):\n",
    "            adj.append(functools.reduce(matmul, (self.adj,) * 2 ** i, adj[-1]))\n",
    "        alpha = self.alpha1(x) + self.alpha2(x) + self.alpha3(x) + \\\n",
    "            sum(getattr(self, 'alpha%d' % (i + 4))(a) for i, a in enumerate(adj))\n",
    "        alpha = self.bn_alpha(self.activation(alpha), training=training)\n",
    "        beta = self.beta1(x) + self.beta2(x) + self.beta3(x) + \\\n",
    "            sum(getattr(self, 'beta%d' % (i + 4))(a) for i, a in enumerate(adj))\n",
    "        beta = self.bn_beta(beta, training=training)\n",
    "        return tf.concat((alpha, beta), 1)\n",
    "\n",
    "class EdgeDense(tf.keras.Model):\n",
    "    def __init__(self, in_features, out_features, adj):\n",
    "        super().__init__()\n",
    "        adj = sp.sparse.triu(adj)\n",
    "        self.adj = tf.cast(my.sparse_sp2tf(adj), tf.float32)\n",
    "        self.out_features = out_features\n",
    "        for i in range(out_features):\n",
    "            setattr(self, 'dense%d' % i, tf.keras.layers.Dense(input_shape=(in_features,), units=1))\n",
    "    \n",
    "    def call(self, x):\n",
    "        z_list = []\n",
    "        for i in range(self.out_features):\n",
    "            z = getattr(self, 'dense%d' % i)(x)\n",
    "            u = z * self.adj\n",
    "            v = tf.transpose(z) * self.adj\n",
    "            z = u.values + v.values\n",
    "            z_list.append(tf.reshape(z, (-1, 1)))\n",
    "        z = tf.concat(z_list, 1)\n",
    "        return z\n",
    "\n",
    "class GNN(tf.keras.Model):\n",
    "    def __init__(self, features, n_machines, adj, radius, activation=tf.keras.activations.relu):\n",
    "        super().__init__()\n",
    "        self.dense = EdgeDense(features[-1], n_machines, adj)\n",
    "        deg = tf.constant(adj.sum(1), dtype=tf.float32)\n",
    "        adj = tf.cast(my.sparse_sp2tf(adj), tf.float32)\n",
    "        for i, (m, n) in enumerate(zip(features[:-1], features[1:])):\n",
    "            setattr(self, 'layer%d' % i, GNNModule(m, n, adj, deg, radius, activation))\n",
    "        self.n_layers = i + 1\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        for i in range(self.n_layers):\n",
    "            x = getattr(self, 'layer%d' % i)(x, training)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = my.read_edgelist('data/' + args.graph)\n",
    "adj = nx.adj_matrix(g)\n",
    "# x = sp.sparse.random(1000, 1000, 1e-5, data_rvs=lambda shape: np.ones(shape))\n",
    "# adj = (x + x.transpose()).minimum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(args.device):\n",
    "    objective = Objective(adj, args.n_machines, args.lambda_node, args.lambda_edge, args.lambda_entropy)\n",
    "    gnn = GNN((1,) + (args.n_features,) * args.depth, args.n_machines, adj, args.radius)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(args.device):\n",
    "    x = tf.constant(adj.sum(1), dtype=tf.float32)\n",
    "    x -= tf.reduce_mean(x)\n",
    "    x /= tf.sqrt(tf.reduce_mean(tf.square(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step=tf.train.get_or_create_global_step()\n",
    "with tf.device(args.device):\n",
    "    for i in range(args.n_iterations):\n",
    "        global_step.assign_add(1)\n",
    "        with tf.contrib.summary.record_summaries_every_n_global_steps(1):\n",
    "            with eager.GradientTape() as tape:\n",
    "                z = gnn(x, training=True)\n",
    "                rslt, r, b_node, b_edge, entropy = objective(z, training=True)\n",
    "            gradients = tape.gradient(rslt, gnn.variables)\n",
    "            optimizer.apply_gradients(zip(gradients, gnn.variables)) # TODO tf.train.get_or_create_global_step\n",
    "\n",
    "            r, b_node, b_edge, entropy = objective(z)\n",
    "            print('[iteration %d]%f %f %f %f %f' % (i + 1, rslt, r, b_node, b_edge, entropy))\n",
    "            tf.contrib.summary.scalar('objective', rslt)\n",
    "            tf.contrib.summary.scalar('replication-factor', r)\n",
    "            tf.contrib.summary.scalar('node-balancedness', b_node)\n",
    "            tf.contrib.summary.scalar('edge-balancedness', b_edge)\n",
    "            tf.contrib.summary.scalar('entropy', entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
