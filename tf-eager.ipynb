{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import networkx as nx\n",
    "import tensorflow as tf # TODO use gpu\n",
    "import tensorflow.contrib.eager as eager\n",
    "tf.enable_eager_execution()\n",
    "import my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.depth = 10\n",
    "args.device = '/cpu:0'\n",
    "# args.device = '/device:GPU:0'\n",
    "args.graph = 'soc-Epinions1'\n",
    "args.n_features = 8\n",
    "args.n_machines = 10\n",
    "args.radius = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    def __init__(self, adj, n_machines):\n",
    "        adj = tf.cast(my.sparse_sp2tf(adj), tf.float32)\n",
    "        n_nodes, n_edges = adj.dense_shape[0], adj.indices.shape[0]\n",
    "        e_idx = tf.expand_dims(tf.range(0, n_edges, dtype=tf.int64), 1)\n",
    "        idx0 = tf.concat((tf.expand_dims(adj.indices[:, 0], 1), e_idx), 1)\n",
    "        idx1 = tf.concat((tf.expand_dims(adj.indices[:, 1], 1), e_idx), 1)\n",
    "        idx = tf.concat((idx0, idx1), 0)\n",
    "        self.s = tf.SparseTensor(idx, tf.ones(idx.shape[0]), dense_shape=(n_nodes, n_edges))\n",
    "        self.n_machines = n_machines\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = tf.multinomial(x, num_samples=1)\n",
    "        y = tf.squeeze(y)\n",
    "        y = tf.one_hot(y, self.n_machines)\n",
    "        z = tf.sparse_tensor_dense_matmul(self.s, y)\n",
    "        # TODO capping\n",
    "        z = tf.sigmoid(z)\n",
    "        r = tf.reduce_sum(z)\n",
    "        p = (tf.reduce_sum(z, 1) + 1) / r\n",
    "        b = -tf.reduce_sum(p * tf.log(p))\n",
    "        objective = -tf.reduce_sum((r + b) * y * tf.log(x))\n",
    "        return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModule(tf.keras.Model):\n",
    "    def __init__(self, in_features, out_features, adj, deg, radius, activation):\n",
    "        super().__init__()\n",
    "        self.adj, self.deg, self.radius = adj, deg, radius\n",
    "        new_dense = lambda: tf.keras.layers.Dense(input_shape=(in_features,), units=out_features, use_bias=False)\n",
    "        self.alpha1, self.alpha2, self.alpha3 = new_dense(), new_dense(), new_dense()\n",
    "        for i in range(radius):\n",
    "            setattr(self, 'alpha%d' % (i + 4), new_dense())\n",
    "        self.beta1, self.beta2, self.beta3 = new_dense(), new_dense(), new_dense()\n",
    "        for i in range(radius):\n",
    "            setattr(self, 'beta%d' % (i + 4), new_dense())\n",
    "        self.bn_alpha, self.bn_beta = tf.keras.layers.BatchNormalization(axis=1), tf.keras.layers.BatchNormalization(axis=1)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        deg = self.deg * x\n",
    "        u = tf.reduce_mean(x, 1, keepdims=True) + tf.zeros_like(x)\n",
    "        adj = [tf.sparse_tensor_dense_matmul(self.adj, x)]\n",
    "        matmul = lambda x, y: tf.sparse_tensor_dense_matmul(y, x)\n",
    "        for i in range(self.radius - 1):\n",
    "            adj.append(functools.reduce(matmul, (self.adj,) * 2 ** i, adj[-1]))\n",
    "        alpha = self.alpha1(x) + self.alpha2(x) + self.alpha3(x) + \\\n",
    "            sum(getattr(self, 'alpha%d' % (i + 4))(a) for i, a in enumerate(adj))\n",
    "        alpha = self.bn_alpha(self.activation(alpha), training=training)\n",
    "        beta = self.beta1(x) + self.beta2(x) + self.beta3(x) + \\\n",
    "            sum(getattr(self, 'beta%d' % (i + 4))(a) for i, a in enumerate(adj))\n",
    "        beta = self.bn_beta(beta, training=training)\n",
    "        return tf.concat((alpha, beta), 1)\n",
    "\n",
    "class EdgeDense(tf.keras.Model):\n",
    "    def __init__(self, in_features, out_features, adj):\n",
    "        super().__init__()\n",
    "        self.adj = adj\n",
    "        self.out_features = out_features\n",
    "        for i in range(out_features):\n",
    "            setattr(self, 'dense%d' % i, tf.keras.layers.Dense(input_shape=(in_features,), units=1))\n",
    "    \n",
    "    def call(self, x):\n",
    "        z_list = []\n",
    "        for i in range(self.out_features):\n",
    "            z = getattr(self, 'dense%d' % i)(x)\n",
    "            u = z * self.adj\n",
    "            v = tf.transpose(z) * self.adj\n",
    "            z = u.values + v.values\n",
    "#             z = tf.sparse_add(z * self.adj, tf.transpose(z) * self.adj).values\n",
    "            z_list.append(tf.reshape(z, (-1, 1)))\n",
    "        z = tf.concat(z_list, 1)\n",
    "        return z\n",
    "\n",
    "class GNN(tf.keras.Model):\n",
    "    def __init__(self, features, n_machines, adj, radius, activation=tf.keras.activations.relu):\n",
    "        super().__init__()\n",
    "        deg = tf.constant(adj.sum(1), dtype=tf.float32)\n",
    "        adj = tf.cast(my.sparse_sp2tf(adj), tf.float32)\n",
    "        for i, (m, n) in enumerate(zip(features[:-1], features[1:])):\n",
    "            setattr(self, 'layer%d' % i, GNNModule(m, n, adj, deg, radius, activation))\n",
    "        self.n_layers = i + 1\n",
    "        self.dense = EdgeDense(features[-1], n_machines, adj)\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        for i in range(self.n_layers):\n",
    "            x = getattr(self, 'layer%d' % i)(x, training)\n",
    "        x = self.dense(x)\n",
    "        x = tf.exp(x) / tf.reduce_sum(tf.exp(x), 1, keepdims=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = my.read_edgelist(args.graph)\n",
    "adj = nx.adj_matrix(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(args.device):\n",
    "    objective = Objective(adj, args.n_machines)\n",
    "    gnn = GNN((1,) + (args.n_features,) * args.depth, args.n_machines, adj, args.radius)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(args.device):\n",
    "    for i in range(100):\n",
    "        x = tf.constant(adj.sum(1), dtype=tf.float32)\n",
    "        x -= tf.reduce_mean(x)\n",
    "        x /= tf.sqrt(tf.reduce_mean(tf.square(x)))\n",
    "        with eager.GradientTape() as tape:\n",
    "            x = gnn(x, training=True)\n",
    "            x = objective(x)\n",
    "            print(x)\n",
    "        gradients = tape.gradient(x, gnn.variables)\n",
    "        optimizer.apply_gradients(zip(gradients, gnn.variables)) # TODO tf.train.get_or_create_global_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
