{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaiyu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as eager\n",
    "tf.enable_eager_execution()\n",
    "import my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.depth = 10\n",
    "args.device = '/cpu:0'\n",
    "args.eta = 0\n",
    "# args.device = '/device:GPU:0' # TODO gpu\n",
    "args.graph = 'soc-Epinions1'\n",
    "# args.graph = 'soc-Slashdot0811'\n",
    "# args.graph = 'soc-Slashdot0902'\n",
    "args.n_features = 8\n",
    "args.n_iterations = 1000\n",
    "args.n_machines = 10\n",
    "args.radius = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    def __init__(self, adj, n_machines, eta):\n",
    "        adj = sp.sparse.triu(adj)\n",
    "        self.n_nodes, self.n_edges = adj.shape[0], len(adj.data)\n",
    "        adj = my.sparse_sp2tf(adj)\n",
    "        adj = tf.cast(adj, tf.float32)\n",
    "        e_idx = tf.expand_dims(tf.range(0, self.n_edges, dtype=tf.int64), 1)\n",
    "        idx0 = tf.concat((tf.expand_dims(adj.indices[:, 0], 1), e_idx), 1)\n",
    "        idx1 = tf.concat((tf.expand_dims(adj.indices[:, 1], 1), e_idx), 1)\n",
    "        idx = tf.concat((idx0, idx1), 0)\n",
    "        self.s = tf.SparseTensor(idx, tf.ones(idx.shape[0]), dense_shape=(self.n_nodes, self.n_edges))\n",
    "        self.n_machines = n_machines\n",
    "        self.eta = eta\n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        # TODO incorporate extent of concentration in objective\n",
    "        # TODO return expected number of edges per machine\n",
    "\n",
    "        p = tf.nn.softmax(x)\n",
    "        \n",
    "        if training:\n",
    "            y = tf.multinomial(p, num_samples=1)\n",
    "            y = tf.squeeze(y)\n",
    "            y = tf.one_hot(y, self.n_machines)\n",
    "        else:\n",
    "            y = p\n",
    "\n",
    "        z = tf.sparse_tensor_dense_matmul(self.s, y)\n",
    "        z = tf.minimum(z, 1)\n",
    "\n",
    "        sum_z = tf.reduce_sum(z)\n",
    "        r = sum_z / float(self.n_nodes)\n",
    "        q = (tf.reduce_sum(z, 0) + 1) / sum_z\n",
    "        b = tf.reduce_sum(q * tf.log(q))\n",
    "\n",
    "        if training:\n",
    "            joint = r\n",
    "#             joint = r + b\n",
    "#             joint = self.eta * r + (1 - self.eta) * b\n",
    "            log_p = tf.nn.log_softmax(x)\n",
    "            objective = joint * tf.reduce_sum(y * log_p) / float(self.n_edges)\n",
    "            return objective, r, b\n",
    "        else:\n",
    "            return r, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModule(tf.keras.Model):\n",
    "    def __init__(self, in_features, out_features, adj, deg, radius, activation):\n",
    "        super().__init__()\n",
    "        self.adj, self.deg, self.radius = adj, deg, radius\n",
    "        new_dense = lambda: tf.keras.layers.Dense(input_shape=(in_features,), units=out_features, use_bias=False)\n",
    "        self.alpha1, self.alpha2, self.alpha3 = new_dense(), new_dense(), new_dense()\n",
    "        for i in range(radius):\n",
    "            setattr(self, 'alpha%d' % (i + 4), new_dense())\n",
    "        self.beta1, self.beta2, self.beta3 = new_dense(), new_dense(), new_dense()\n",
    "        for i in range(radius):\n",
    "            setattr(self, 'beta%d' % (i + 4), new_dense())\n",
    "        self.bn_alpha, self.bn_beta = tf.keras.layers.BatchNormalization(axis=1), tf.keras.layers.BatchNormalization(axis=1)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        deg = self.deg * x\n",
    "        u = tf.reduce_mean(x, 1, keepdims=True) + tf.zeros_like(x)\n",
    "        adj = [tf.sparse_tensor_dense_matmul(self.adj, x)]\n",
    "        matmul = lambda x, y: tf.sparse_tensor_dense_matmul(y, x)\n",
    "        for i in range(self.radius - 1):\n",
    "            adj.append(functools.reduce(matmul, (self.adj,) * 2 ** i, adj[-1]))\n",
    "        alpha = self.alpha1(x) + self.alpha2(x) + self.alpha3(x) + \\\n",
    "            sum(getattr(self, 'alpha%d' % (i + 4))(a) for i, a in enumerate(adj))\n",
    "        alpha = self.bn_alpha(self.activation(alpha), training=training)\n",
    "        beta = self.beta1(x) + self.beta2(x) + self.beta3(x) + \\\n",
    "            sum(getattr(self, 'beta%d' % (i + 4))(a) for i, a in enumerate(adj))\n",
    "        beta = self.bn_beta(beta, training=training)\n",
    "        return tf.concat((alpha, beta), 1)\n",
    "\n",
    "class EdgeDense(tf.keras.Model):\n",
    "    def __init__(self, in_features, out_features, adj):\n",
    "        super().__init__()\n",
    "        adj = sp.sparse.triu(adj)\n",
    "        self.adj = tf.cast(my.sparse_sp2tf(adj), tf.float32)\n",
    "        self.out_features = out_features\n",
    "        for i in range(out_features):\n",
    "            setattr(self, 'dense%d' % i, tf.keras.layers.Dense(input_shape=(in_features,), units=1))\n",
    "    \n",
    "    def call(self, x):\n",
    "        z_list = []\n",
    "        for i in range(self.out_features):\n",
    "            z = getattr(self, 'dense%d' % i)(x)\n",
    "            u = z * self.adj\n",
    "            v = tf.transpose(z) * self.adj\n",
    "            z = u.values + v.values\n",
    "            z_list.append(tf.reshape(z, (-1, 1)))\n",
    "        z = tf.concat(z_list, 1)\n",
    "        return z\n",
    "\n",
    "class GNN(tf.keras.Model):\n",
    "    def __init__(self, features, n_machines, adj, radius, activation=tf.keras.activations.relu):\n",
    "        super().__init__()\n",
    "        self.dense = EdgeDense(features[-1], n_machines, adj)\n",
    "        deg = tf.constant(adj.sum(1), dtype=tf.float32)\n",
    "        adj = tf.cast(my.sparse_sp2tf(adj), tf.float32)\n",
    "        for i, (m, n) in enumerate(zip(features[:-1], features[1:])):\n",
    "            setattr(self, 'layer%d' % i, GNNModule(m, n, adj, deg, radius, activation))\n",
    "        self.n_layers = i + 1\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        for i in range(self.n_layers):\n",
    "            x = getattr(self, 'layer%d' % i)(x, training)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = my.read_edgelist('data/' + args.graph)\n",
    "adj = nx.adj_matrix(g)\n",
    "# x = sp.sparse.random(1000, 1000, 1e-5, data_rvs=lambda shape: np.ones(shape))\n",
    "# adj = (x + x.transpose()).minimum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(args.device):\n",
    "    objective = Objective(adj, args.n_machines, args.eta)\n",
    "    gnn = GNN((1,) + (args.n_features,) * args.depth, args.n_machines, adj, args.radius)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(args.device):\n",
    "    x = tf.constant(adj.sum(1), dtype=tf.float32)\n",
    "    x -= tf.reduce_mean(x)\n",
    "    x /= tf.sqrt(tf.reduce_mean(tf.square(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 1]-42.563362 2.708159 -2.227542\n",
      "[iteration 2]-43.194542 2.699521 -2.226448\n",
      "[iteration 3]-43.571594 2.688580 -2.225018\n",
      "[iteration 4]-47.360310 2.670804 -2.219677\n",
      "[iteration 5]-46.743496 2.508573 -2.212607\n",
      "[iteration 6]-47.016361 2.510904 -2.213331\n",
      "[iteration 7]-47.463520 2.505195 -2.212242\n",
      "[iteration 8]-47.988914 2.498802 -2.210998\n",
      "[iteration 9]-48.473480 2.491981 -2.209648\n",
      "[iteration 10]-49.076553 2.485011 -2.208218\n",
      "[iteration 11]-49.545200 2.477817 -2.206748\n",
      "[iteration 12]-50.135265 2.470615 -2.205253\n",
      "[iteration 13]-50.892166 2.463284 -2.203674\n",
      "[iteration 14]-47.806210 2.551860 -2.222705\n",
      "[iteration 15]-45.498005 2.571074 -2.225294\n",
      "[iteration 16]-48.157257 2.545171 -2.221474\n",
      "[iteration 17]-41.379639 2.582226 -2.226843\n",
      "[iteration 18]-48.828384 2.535567 -2.219660\n",
      "[iteration 19]-49.128162 2.529846 -2.218615\n",
      "[iteration 20]-49.580154 2.524021 -2.217482\n",
      "[iteration 21]-49.994381 2.517664 -2.216252\n",
      "[iteration 22]-51.461346 2.443587 -2.201334\n",
      "[iteration 23]-53.978809 2.454893 -2.204890\n",
      "[iteration 24]-54.437546 2.450740 -2.203981\n",
      "[iteration 25]-54.714680 2.446195 -2.202999\n",
      "[iteration 26]-55.230629 2.441325 -2.201959\n",
      "[iteration 27]-55.548977 2.436302 -2.200871\n",
      "[iteration 28]-56.198219 2.431079 -2.199763\n",
      "[iteration 29]-56.522186 2.425754 -2.198577\n",
      "[iteration 30]-57.017746 2.420303 -2.197374\n",
      "[iteration 31]-57.649628 2.414722 -2.196135\n",
      "[iteration 32]-58.194214 2.409171 -2.194869\n",
      "[iteration 33]-58.715748 2.403516 -2.193589\n",
      "[iteration 34]-59.145779 2.397809 -2.192283\n",
      "[iteration 35]-59.681492 2.392098 -2.190970\n",
      "[iteration 36]-60.231468 2.386378 -2.189614\n",
      "[iteration 37]-60.846531 2.381168 -2.188350\n",
      "[iteration 38]-57.073589 2.440708 -2.200020\n",
      "[iteration 39]-61.941444 2.370422 -2.185779\n",
      "[iteration 40]-62.264194 2.365307 -2.184561\n",
      "[iteration 41]-63.124004 2.360131 -2.183326\n",
      "[iteration 42]-63.537323 2.354927 -2.182043\n",
      "[iteration 43]-64.097099 2.349698 -2.180760\n",
      "[iteration 44]-64.508713 2.344447 -2.179483\n",
      "[iteration 45]-64.981667 2.339183 -2.178133\n",
      "[iteration 46]-65.659958 2.333896 -2.176764\n",
      "[iteration 47]-66.257057 2.328615 -2.175421\n",
      "[iteration 48]-66.928894 2.323330 -2.174071\n",
      "[iteration 49]-67.498444 2.318049 -2.172690\n",
      "[iteration 50]-67.960861 2.312782 -2.171296\n",
      "[iteration 51]-68.671593 2.307527 -2.169903\n",
      "[iteration 52]-69.223701 2.302297 -2.168507\n",
      "[iteration 53]-69.859367 2.297091 -2.167105\n",
      "[iteration 54]-70.552490 2.291907 -2.165664\n",
      "[iteration 55]-70.956795 2.286745 -2.164225\n",
      "[iteration 56]-71.452133 2.281615 -2.162803\n",
      "[iteration 57]-72.494865 2.276508 -2.161349\n",
      "[iteration 58]-73.078461 2.271422 -2.159894\n",
      "[iteration 59]-73.642799 2.266360 -2.158475\n",
      "[iteration 60]-74.176201 2.261323 -2.156990\n",
      "[iteration 61]-74.588608 2.256301 -2.155515\n",
      "[iteration 62]-75.439095 2.251306 -2.154063\n",
      "[iteration 63]-76.195389 2.246343 -2.152570\n",
      "[iteration 64]-76.584000 2.241408 -2.151072\n",
      "[iteration 65]-77.261673 2.236504 -2.149581\n",
      "[iteration 66]-78.023407 2.231624 -2.148071\n",
      "[iteration 67]-78.346291 2.226766 -2.146562\n",
      "[iteration 68]-79.351761 2.221938 -2.145051\n",
      "[iteration 69]-79.847038 2.217128 -2.143526\n",
      "[iteration 70]-80.408043 2.212336 -2.142004\n",
      "[iteration 71]-81.092743 2.207567 -2.140502\n",
      "[iteration 72]-81.500870 2.202814 -2.138958\n",
      "[iteration 73]-82.279106 2.198077 -2.137394\n",
      "[iteration 74]-82.914566 2.193368 -2.135857\n",
      "[iteration 75]-83.502823 2.188680 -2.134271\n",
      "[iteration 76]-84.252853 2.184015 -2.132674\n",
      "[iteration 77]-84.771759 2.179362 -2.131094\n",
      "[iteration 78]-85.617516 2.174730 -2.129516\n",
      "[iteration 79]-86.264404 2.170114 -2.127901\n",
      "[iteration 80]-86.479965 2.165516 -2.126285\n",
      "[iteration 81]-87.490067 2.160938 -2.124686\n",
      "[iteration 82]-88.017105 2.156378 -2.123057\n",
      "[iteration 83]-88.502762 2.151832 -2.121422\n",
      "[iteration 84]-89.163017 2.147305 -2.119772\n",
      "[iteration 85]-90.005226 2.142790 -2.118134\n",
      "[iteration 86]-90.497070 2.138289 -2.116465\n",
      "[iteration 87]-91.144440 2.133801 -2.114802\n",
      "[iteration 88]-91.816040 2.129330 -2.113132\n",
      "[iteration 89]-92.589424 2.124871 -2.111441\n",
      "[iteration 90]-93.237770 2.120426 -2.109722\n",
      "[iteration 91]-93.973000 2.115994 -2.108004\n",
      "[iteration 92]-94.847664 2.111570 -2.106289\n",
      "[iteration 93]-95.121277 2.107148 -2.104543\n",
      "[iteration 94]-96.022781 2.102736 -2.102793\n",
      "[iteration 95]-96.302765 2.098337 -2.100994\n",
      "[iteration 96]-97.287788 2.093946 -2.099243\n",
      "[iteration 97]-97.722374 2.089568 -2.097448\n",
      "[iteration 98]-98.291656 2.085200 -2.095643\n",
      "[iteration 99]-98.977669 2.080847 -2.093820\n",
      "[iteration 100]-99.574440 2.076507 -2.092018\n",
      "[iteration 101]-100.387054 2.072177 -2.090189\n",
      "[iteration 102]-100.839699 2.067856 -2.088331\n",
      "[iteration 103]-101.725914 2.063544 -2.086474\n",
      "[iteration 104]-102.584572 2.059243 -2.084615\n",
      "[iteration 105]-103.069565 2.054953 -2.082741\n",
      "[iteration 106]-103.735695 2.050667 -2.080859\n",
      "[iteration 107]-104.752785 2.046388 -2.078931\n",
      "[iteration 108]-105.351677 2.042112 -2.076996\n",
      "[iteration 109]-106.064812 2.037850 -2.075062\n",
      "[iteration 110]-106.310516 2.033601 -2.073102\n",
      "[iteration 111]-107.379898 2.029367 -2.071136\n",
      "[iteration 112]-108.082069 2.025142 -2.069164\n",
      "[iteration 113]-108.458435 2.020927 -2.067176\n",
      "[iteration 114]-109.257111 2.016717 -2.065180\n",
      "[iteration 115]-109.923409 2.012516 -2.063152\n",
      "[iteration 116]-110.401062 2.008318 -2.061140\n",
      "[iteration 117]-111.465424 2.004124 -2.059087\n",
      "[iteration 118]-111.933365 1.999929 -2.057006\n",
      "[iteration 119]-112.636818 1.995731 -2.054936\n",
      "[iteration 120]-113.589241 1.991541 -2.052834\n",
      "[iteration 121]-113.900436 1.987349 -2.050698\n",
      "[iteration 122]-114.875938 1.983158 -2.048557\n",
      "[iteration 123]-115.577309 1.978970 -2.046408\n",
      "[iteration 124]-116.126038 1.974784 -2.044220\n",
      "[iteration 125]-117.296257 1.970607 -2.042020\n",
      "[iteration 126]-117.632996 1.966433 -2.039820\n",
      "[iteration 127]-118.128181 1.962264 -2.037582\n",
      "[iteration 128]-119.068604 1.958094 -2.035301\n",
      "[iteration 129]-119.659950 1.953924 -2.033019\n",
      "[iteration 130]-120.441399 1.949758 -2.030721\n",
      "[iteration 131]-121.403778 1.945592 -2.028404\n",
      "[iteration 132]-121.552071 1.941425 -2.026062\n",
      "[iteration 133]-123.016731 1.937258 -2.023723\n",
      "[iteration 134]-122.979309 1.933091 -2.021365\n",
      "[iteration 135]-124.043083 1.928926 -2.018962\n",
      "[iteration 136]-125.164314 1.924763 -2.016528\n",
      "[iteration 137]-125.748344 1.920596 -2.014075\n",
      "[iteration 138]-126.021149 1.916428 -2.011631\n",
      "[iteration 139]-126.735001 1.912263 -2.009136\n",
      "[iteration 140]-127.405899 1.908095 -2.006575\n",
      "[iteration 141]-128.068451 1.903923 -2.004042\n",
      "[iteration 142]-129.241608 1.899754 -2.001506\n",
      "[iteration 143]-129.626862 1.895586 -1.998939\n",
      "[iteration 144]-130.475204 1.891418 -1.996311\n",
      "[iteration 145]-131.391357 1.887244 -1.993659\n",
      "[iteration 146]-131.906296 1.883073 -1.991001\n",
      "[iteration 147]-132.627029 1.878901 -1.988327\n",
      "[iteration 148]-133.564911 1.874723 -1.985621\n",
      "[iteration 149]-134.513885 1.870544 -1.982852\n",
      "[iteration 150]-134.782211 1.866362 -1.980073\n",
      "[iteration 151]-135.810089 1.862181 -1.977287\n",
      "[iteration 152]-136.279968 1.857998 -1.974470\n",
      "[iteration 153]-137.365768 1.853817 -1.971600\n",
      "[iteration 154]-138.462341 1.849632 -1.968741\n",
      "[iteration 155]-138.875137 1.845443 -1.965829\n",
      "[iteration 156]-139.340302 1.841251 -1.962851\n",
      "[iteration 157]-140.511917 1.837057 -1.959904\n",
      "[iteration 158]-140.939606 1.832858 -1.956909\n",
      "[iteration 159]-141.866501 1.828655 -1.953862\n",
      "[iteration 160]-142.800323 1.824445 -1.950800\n",
      "[iteration 161]-143.937531 1.820233 -1.947721\n",
      "[iteration 162]-144.070938 1.816014 -1.944570\n",
      "[iteration 163]-144.927078 1.811795 -1.941403\n",
      "[iteration 164]-145.478806 1.807567 -1.938224\n",
      "[iteration 165]-146.131271 1.803336 -1.935002\n",
      "[iteration 166]-147.079910 1.799104 -1.931722\n",
      "[iteration 167]-148.127853 1.794870 -1.928456\n",
      "[iteration 168]-148.712997 1.790625 -1.925105\n",
      "[iteration 169]-149.714157 1.786373 -1.921733\n",
      "[iteration 170]-150.639221 1.782115 -1.918333\n",
      "[iteration 171]-151.152512 1.777853 -1.914881\n",
      "[iteration 172]-151.996994 1.773587 -1.911395\n",
      "[iteration 173]-152.719788 1.769317 -1.907850\n",
      "[iteration 174]-153.915344 1.765041 -1.904296\n",
      "[iteration 175]-154.151215 1.760764 -1.900716\n",
      "[iteration 176]-155.029968 1.756480 -1.897062\n",
      "[iteration 177]-155.905121 1.752194 -1.893368\n",
      "[iteration 178]-156.886383 1.747902 -1.889661\n",
      "[iteration 179]-157.396423 1.743603 -1.885914\n",
      "[iteration 180]-158.317627 1.739298 -1.882108\n",
      "[iteration 181]-159.393677 1.734989 -1.878217\n",
      "[iteration 182]-159.908432 1.730672 -1.874351\n",
      "[iteration 183]-160.654663 1.726351 -1.870407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 184]-161.413681 1.722023 -1.866414\n",
      "[iteration 185]-162.082855 1.717688 -1.862392\n",
      "[iteration 186]-163.031357 1.713348 -1.858332\n",
      "[iteration 187]-164.051819 1.708998 -1.854194\n",
      "[iteration 188]-164.912994 1.704646 -1.850022\n",
      "[iteration 189]-165.518921 1.700287 -1.845815\n",
      "[iteration 190]-166.780441 1.695918 -1.841537\n",
      "[iteration 191]-167.483643 1.691538 -1.837224\n",
      "[iteration 192]-167.838760 1.687149 -1.832848\n",
      "[iteration 193]-169.046005 1.682753 -1.828408\n",
      "[iteration 194]-169.486435 1.678351 -1.823952\n",
      "[iteration 195]-170.531967 1.673942 -1.819413\n",
      "[iteration 196]-171.455338 1.669527 -1.814837\n",
      "[iteration 197]-172.222534 1.665104 -1.810211\n",
      "[iteration 198]-173.125641 1.660673 -1.805510\n",
      "[iteration 199]-86.256516 1.665199 -1.531325\n",
      "[iteration 200]-174.479019 1.652021 -1.796261\n",
      "[iteration 201]-175.926849 1.647779 -1.791649\n",
      "[iteration 202]-176.352951 1.643505 -1.786953\n",
      "[iteration 203]-176.882950 1.639197 -1.782191\n",
      "[iteration 204]-177.969360 1.634874 -1.777344\n",
      "[iteration 205]-178.633957 1.630519 -1.772438\n",
      "[iteration 206]-179.709671 1.626139 -1.767416\n",
      "[iteration 207]-180.338013 1.621743 -1.762368\n",
      "[iteration 208]-180.881668 1.617321 -1.757202\n",
      "[iteration 209]-181.877228 1.612882 -1.751983\n",
      "[iteration 210]-182.211639 1.608416 -1.746658\n",
      "[iteration 211]-183.367188 1.603939 -1.741290\n",
      "[iteration 212]-184.429352 1.599442 -1.735824\n",
      "[iteration 213]-185.280945 1.594928 -1.730289\n",
      "[iteration 214]-186.251999 1.590407 -1.724694\n",
      "[iteration 215]-186.814636 1.585866 -1.719004\n",
      "[iteration 216]-188.086319 1.581313 -1.713242\n",
      "[iteration 217]-188.610596 1.576739 -1.707402\n",
      "[iteration 218]-189.554947 1.572157 -1.701476\n",
      "[iteration 219]-190.409943 1.567562 -1.695454\n",
      "[iteration 220]-191.498245 1.562960 -1.689382\n",
      "[iteration 221]-193.125687 1.558345 -1.683227\n",
      "[iteration 222]-193.232193 1.553716 -1.676993\n",
      "[iteration 223]-194.031113 1.549072 -1.670665\n",
      "[iteration 224]-194.960800 1.544411 -1.664246\n",
      "[iteration 225]-196.067413 1.539745 -1.657734\n",
      "[iteration 226]-196.957382 1.535071 -1.651176\n",
      "[iteration 227]-197.176208 1.530382 -1.644491\n",
      "[iteration 228]-198.003555 1.525683 -1.637732\n",
      "[iteration 229]-198.933136 1.520972 -1.630906\n",
      "[iteration 230]-199.953232 1.516246 -1.623955\n",
      "[iteration 231]-201.240097 1.511514 -1.616933\n",
      "[iteration 232]-202.021072 1.506768 -1.609824\n",
      "[iteration 233]-202.977585 1.502017 -1.602587\n",
      "[iteration 234]-203.203354 1.497261 -1.595286\n",
      "[iteration 235]-204.525269 1.492494 -1.587885\n",
      "[iteration 236]-205.474930 1.487714 -1.580382\n",
      "[iteration 237]-206.254807 1.482929 -1.572784\n",
      "[iteration 238]-207.607269 1.478135 -1.565084\n",
      "[iteration 239]-208.683395 1.473333 -1.557299\n",
      "[iteration 240]-209.575439 1.468520 -1.549400\n",
      "[iteration 241]-210.623489 1.463695 -1.541403\n",
      "[iteration 242]-211.570511 1.458871 -1.533331\n",
      "[iteration 243]-212.276505 1.454031 -1.525105\n",
      "[iteration 244]-213.693130 1.449186 -1.516782\n",
      "[iteration 245]-213.766708 1.444327 -1.508381\n",
      "[iteration 246]-214.609146 1.439462 -1.499839\n",
      "[iteration 247]-216.062088 1.434595 -1.491209\n",
      "[iteration 248]-216.723633 1.429718 -1.482467\n",
      "[iteration 249]-218.330872 1.424836 -1.473626\n",
      "[iteration 250]-218.812820 1.419934 -1.464668\n",
      "[iteration 251]-219.699570 1.415012 -1.455585\n",
      "[iteration 252]-220.178818 1.410089 -1.446399\n",
      "[iteration 253]-222.010147 1.405159 -1.437102\n",
      "[iteration 254]-222.200684 1.400236 -1.427693\n",
      "[iteration 255]-223.644211 1.395307 -1.418171\n",
      "[iteration 256]-224.922501 1.390372 -1.408539\n",
      "[iteration 257]-225.169464 1.385438 -1.398768\n",
      "[iteration 258]-226.016190 1.380495 -1.388896\n",
      "[iteration 259]-227.686386 1.375550 -1.378921\n",
      "[iteration 260]-228.308350 1.370608 -1.368782\n",
      "[iteration 261]-229.765747 1.365660 -1.358576\n",
      "[iteration 262]-230.669220 1.360715 -1.348227\n",
      "[iteration 263]-231.074722 1.355770 -1.337771\n",
      "[iteration 264]-232.329681 1.350817 -1.327176\n",
      "[iteration 265]-233.635406 1.345863 -1.316478\n",
      "[iteration 266]-233.923737 1.340905 -1.305673\n",
      "[iteration 267]-234.913116 1.335942 -1.294757\n",
      "[iteration 268]-236.738937 1.330979 -1.283713\n",
      "[iteration 269]-237.505753 1.326024 -1.272546\n",
      "[iteration 270]-238.220596 1.321074 -1.261261\n",
      "[iteration 271]-239.132446 1.316127 -1.249845\n",
      "[iteration 272]-239.535583 1.311206 -1.238347\n",
      "[iteration 273]-241.402618 1.306313 -1.226798\n",
      "[iteration 274]-242.106506 1.301380 -1.214975\n",
      "[iteration 275]-243.405151 1.296475 -1.203084\n",
      "[iteration 276]-244.451065 1.291610 -1.191169\n",
      "[iteration 277]-245.255234 1.286764 -1.179181\n",
      "[iteration 278]-246.421783 1.281894 -1.167002\n",
      "[iteration 279]-246.834290 1.277032 -1.154673\n",
      "[iteration 280]-248.146652 1.272266 -1.142473\n",
      "[iteration 281]-248.639175 1.267488 -1.130092\n",
      "[iteration 282]-250.322037 1.262815 -1.117920\n",
      "[iteration 283]-251.529846 1.258206 -1.105859\n",
      "[iteration 284]-252.631912 1.253944 -1.094959\n",
      "[iteration 285]-205.199112 1.138960 -0.714147\n",
      "[iteration 286]-254.617111 1.244954 -1.070651\n",
      "[iteration 287]-255.502472 1.239496 -1.056053\n",
      "[iteration 288]-256.477325 1.234429 -1.040655\n",
      "[iteration 289]-257.507599 1.229688 -1.027896\n",
      "[iteration 290]-257.281738 1.221980 -1.002283\n",
      "[iteration 291]-259.804230 1.220155 -1.000606\n",
      "[iteration 292]-260.502594 1.216633 -0.990065\n",
      "[iteration 293]-188.284576 1.074234 -0.434256\n",
      "[iteration 294]-189.058884 1.072195 -0.426476\n",
      "[iteration 295]-190.679871 1.070230 -0.417635\n",
      "[iteration 296]-190.786194 1.068381 -0.409345\n",
      "[iteration 297]-192.611862 1.066646 -0.401379\n",
      "[iteration 298]-193.331055 1.065048 -0.394150\n",
      "[iteration 299]-195.005356 1.063496 -0.387077\n",
      "[iteration 300]-195.705170 1.062076 -0.380522\n",
      "[iteration 301]-197.411255 1.060863 -0.375114\n",
      "[iteration 302]-103.828598 1.332618 -0.683344\n",
      "[iteration 303]-193.955536 1.057277 -0.344513\n",
      "[iteration 304]-124.644226 1.096855 -0.374179\n",
      "[iteration 305]-56.508324 1.799525 -1.374820\n",
      "[iteration 306]-56.693459 1.795949 -1.368395\n",
      "[iteration 307]-51.249142 1.845403 -1.485034\n",
      "[iteration 308]-51.495712 1.839434 -1.477739\n",
      "[iteration 309]-51.821308 1.832893 -1.470711\n",
      "[iteration 310]-52.379429 1.825419 -1.463139\n",
      "[iteration 311]-53.036167 1.817097 -1.454926\n",
      "[iteration 312]-53.580368 1.807838 -1.446162\n",
      "[iteration 313]-54.277554 1.797775 -1.436869\n",
      "[iteration 314]-54.783592 1.786949 -1.427065\n",
      "[iteration 315]-55.792572 1.775607 -1.416828\n",
      "[iteration 316]-56.379547 1.763837 -1.406182\n",
      "[iteration 317]-57.192684 1.751503 -1.395013\n",
      "[iteration 318]-57.905121 1.738878 -1.383477\n",
      "[iteration 319]-59.119392 1.725979 -1.371561\n",
      "[iteration 320]-59.955811 1.712790 -1.359248\n",
      "[iteration 321]-60.876644 1.699419 -1.346561\n",
      "[iteration 322]-61.770386 1.686155 -1.333695\n",
      "[iteration 323]-63.077148 1.672673 -1.320069\n",
      "[iteration 324]-64.151833 1.651729 -1.296043\n",
      "[iteration 325]-65.873077 1.554158 -1.266915\n",
      "[iteration 326]-66.384933 1.552287 -1.262948\n",
      "[iteration 327]-67.090317 1.548461 -1.255973\n",
      "[iteration 328]-67.820702 1.543212 -1.246727\n",
      "[iteration 329]-68.587074 1.536907 -1.235746\n",
      "[iteration 330]-69.084076 1.529889 -1.223465\n",
      "[iteration 331]-69.905922 1.522358 -1.210171\n",
      "[iteration 332]-70.962151 1.514547 -1.196177\n",
      "[iteration 333]-71.858368 1.506637 -1.181759\n",
      "[iteration 334]-72.996330 1.498703 -1.166992\n",
      "[iteration 335]-73.777855 1.490844 -1.152065\n",
      "[iteration 336]-74.703896 1.483117 -1.137058\n",
      "[iteration 337]-75.784096 1.475574 -1.122082\n",
      "[iteration 338]-77.042725 1.468245 -1.107198\n",
      "[iteration 339]-77.977745 1.461206 -1.092550\n",
      "[iteration 340]-79.139374 1.454433 -1.078120\n",
      "[iteration 341]-79.979736 1.448005 -1.064067\n",
      "[iteration 342]-80.976044 1.441867 -1.050330\n",
      "[iteration 343]-82.443718 1.436017 -1.036934\n",
      "[iteration 344]-83.557724 1.430465 -1.023931\n",
      "[iteration 345]-84.448593 1.425230 -1.011383\n",
      "[iteration 346]-85.783409 1.420249 -0.999193\n",
      "[iteration 347]-86.825012 1.415537 -0.987410\n",
      "[iteration 348]-87.824295 1.411092 -0.976064\n",
      "[iteration 349]-88.775810 1.406902 -0.965151\n",
      "[iteration 350]-90.260010 1.402958 -0.954671\n",
      "[iteration 351]-91.334694 1.399250 -0.944618\n",
      "[iteration 352]-92.158890 1.395746 -0.934945\n",
      "[iteration 353]-93.244263 1.392435 -0.925641\n",
      "[iteration 354]-94.164131 1.389316 -0.916715\n",
      "[iteration 355]-95.507713 1.386376 -0.908165\n",
      "[iteration 356]-96.573273 1.383605 -0.899967\n",
      "[iteration 357]-97.876839 1.380991 -0.892108\n",
      "[iteration 358]-98.766563 1.378526 -0.884587\n",
      "[iteration 359]-99.634644 1.376199 -0.877380\n",
      "[iteration 360]-100.877022 1.374001 -0.870483\n",
      "[iteration 361]-102.072655 1.371929 -0.863885\n",
      "[iteration 362]-103.030060 1.369973 -0.857571\n",
      "[iteration 363]-103.905518 1.368120 -0.851510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 364]-104.852547 1.366365 -0.845699\n",
      "[iteration 365]-105.990135 1.364704 -0.840123\n",
      "[iteration 366]-107.152611 1.363135 -0.834782\n",
      "[iteration 367]-107.919365 1.361646 -0.829682\n",
      "[iteration 368]-109.028770 1.360237 -0.824814\n",
      "[iteration 369]-110.065735 1.358906 -0.820182\n",
      "[iteration 370]-111.627052 1.357645 -0.815757\n",
      "[iteration 371]-112.376801 1.356447 -0.811503\n",
      "[iteration 372]-113.050079 1.355310 -0.807419\n",
      "[iteration 373]-114.386848 1.354231 -0.803501\n",
      "[iteration 374]-115.134186 1.353206 -0.799743\n",
      "[iteration 375]-116.253624 1.352229 -0.796141\n",
      "[iteration 376]-117.416267 1.351301 -0.792692\n",
      "[iteration 377]-229.739258 1.051787 -0.332067\n",
      "[iteration 378]-119.285027 1.349622 -0.786307\n",
      "[iteration 379]-230.283417 1.051551 -0.330916\n",
      "[iteration 380]-231.241501 1.051343 -0.329906\n",
      "[iteration 381]-231.494812 1.051037 -0.328407\n",
      "[iteration 382]-232.319992 1.050678 -0.326667\n",
      "[iteration 383]-233.159546 1.050411 -0.325530\n",
      "[iteration 384]-279.906403 1.099426 -0.575611\n",
      "[iteration 385]-280.833252 1.097680 -0.567931\n",
      "[iteration 386]-280.737610 1.096144 -0.561277\n",
      "[iteration 387]-283.128143 1.094498 -0.554159\n",
      "[iteration 388]-282.485626 1.092829 -0.546897\n",
      "[iteration 389]-284.101776 1.091149 -0.539613\n",
      "[iteration 390]-284.971924 1.089373 -0.531806\n",
      "[iteration 391]-285.674438 1.087574 -0.523880\n",
      "[iteration 392]-286.917572 1.085745 -0.515732\n",
      "[iteration 393]-287.652527 1.083883 -0.507421\n",
      "[iteration 394]-288.922119 1.082046 -0.499161\n",
      "[iteration 395]-289.940460 1.080317 -0.491403\n",
      "[iteration 396]-291.201782 1.078344 -0.482252\n",
      "[iteration 397]-292.854584 1.076213 -0.472313\n",
      "[iteration 398]-293.209259 1.074267 -0.463253\n",
      "[iteration 399]-294.039246 1.072366 -0.454265\n",
      "[iteration 400]-295.108032 1.070491 -0.445382\n",
      "[iteration 401]-296.403625 1.068642 -0.436506\n",
      "[iteration 402]-297.609192 1.066789 -0.427525\n",
      "[iteration 403]-299.699402 1.065001 -0.418837\n",
      "[iteration 404]-300.588318 1.063277 -0.410309\n",
      "[iteration 405]-302.242157 1.061555 -0.401842\n",
      "[iteration 406]-302.719147 1.059879 -0.393424\n",
      "[iteration 407]-304.025330 1.058242 -0.385262\n",
      "[iteration 408]-305.968628 1.056691 -0.377385\n",
      "[iteration 409]-307.055603 1.055187 -0.369806\n",
      "[iteration 410]-308.647003 1.053866 -0.363199\n",
      "[iteration 411]-309.558563 1.052493 -0.356154\n",
      "[iteration 412]-310.896210 1.050955 -0.348133\n",
      "[iteration 413]-312.297333 1.049212 -0.338641\n",
      "[iteration 414]-314.110321 1.047438 -0.328925\n",
      "[iteration 415]-314.726410 1.045772 -0.319716\n",
      "[iteration 416]-315.636047 1.044183 -0.310841\n",
      "[iteration 417]-317.157990 1.042698 -0.302596\n",
      "[iteration 418]-318.686493 1.041282 -0.294580\n",
      "[iteration 419]-319.806183 1.039959 -0.287179\n",
      "[iteration 420]-321.332520 1.038752 -0.280371\n",
      "[iteration 421]-322.678253 1.038478 -0.279716\n",
      "[iteration 422]-323.062408 1.036100 -0.264958\n",
      "[iteration 423]-325.208405 1.034813 -0.257320\n",
      "[iteration 424]-326.834045 1.033599 -0.250185\n",
      "[iteration 425]-327.717102 1.032473 -0.243432\n",
      "[iteration 426]-329.346069 1.031389 -0.236993\n",
      "[iteration 427]-330.443054 1.030328 -0.230605\n",
      "[iteration 428]-332.484161 1.029276 -0.224139\n",
      "[iteration 429]-332.617493 1.028262 -0.217962\n",
      "[iteration 430]-334.329132 1.027250 -0.211661\n",
      "[iteration 431]-336.159088 1.026215 -0.205073\n",
      "[iteration 432]-337.924988 1.025206 -0.198705\n",
      "[iteration 433]-337.961090 1.024238 -0.192403\n",
      "[iteration 434]-339.407532 1.023300 -0.186378\n",
      "[iteration 435]-341.099396 1.022411 -0.180629\n",
      "[iteration 436]-342.868408 1.021552 -0.174888\n",
      "[iteration 437]-343.974884 1.020737 -0.169529\n",
      "[iteration 438]-344.639313 1.019964 -0.164445\n",
      "[iteration 439]-346.043030 1.019207 -0.159372\n",
      "[iteration 440]-348.042938 1.018472 -0.154344\n",
      "[iteration 441]-349.711273 1.017774 -0.149631\n",
      "[iteration 442]-351.051453 1.017093 -0.144988\n",
      "[iteration 443]-351.358734 1.016424 -0.140354\n",
      "[iteration 444]-353.679535 1.015785 -0.135823\n",
      "[iteration 445]-354.608368 1.015154 -0.131403\n",
      "[iteration 446]-356.051270 1.014546 -0.127122\n",
      "[iteration 447]-358.527527 1.013958 -0.122936\n",
      "[iteration 448]-358.968903 1.013380 -0.118668\n",
      "[iteration 449]-360.845978 1.012836 -0.114745\n",
      "[iteration 450]-361.052582 1.012311 -0.110932\n",
      "[iteration 451]-363.074432 1.011810 -0.107270\n",
      "[iteration 452]-363.777863 1.011324 -0.103678\n",
      "[iteration 453]-366.310608 1.010857 -0.100095\n",
      "[iteration 454]-367.598389 1.010406 -0.096731\n",
      "[iteration 455]-368.467285 1.009966 -0.093410\n",
      "[iteration 456]-370.320892 1.009544 -0.090221\n",
      "[iteration 457]-372.064240 1.009146 -0.087176\n",
      "[iteration 458]-373.402252 1.008769 -0.084262\n",
      "[iteration 459]-375.180756 1.008378 -0.081121\n",
      "[iteration 460]-375.898926 1.007998 -0.078115\n",
      "[iteration 461]-376.790466 1.007647 -0.075315\n",
      "[iteration 462]-302.507446 1.001825 -0.024074\n",
      "[iteration 463]-303.257721 1.001745 -0.023338\n",
      "[iteration 464]-304.622223 1.001661 -0.022568\n",
      "[iteration 465]-306.172363 1.001582 -0.021837\n",
      "[iteration 466]-307.713593 1.001510 -0.021153\n",
      "[iteration 467]-308.727844 1.001440 -0.020477\n",
      "[iteration 468]-309.700867 1.001374 -0.019827\n",
      "[iteration 469]-310.530029 1.001312 -0.019097\n",
      "[iteration 470]-311.982269 1.001252 -0.018515\n",
      "[iteration 471]-312.934296 1.001194 -0.017985\n",
      "[iteration 472]-314.393402 1.001141 -0.017479\n",
      "[iteration 473]-315.892700 1.001090 -0.016997\n",
      "[iteration 474]-317.222137 1.001042 -0.016536\n",
      "[iteration 475]-318.558929 1.000997 -0.016098\n",
      "[iteration 476]-320.983124 1.000952 -0.015674\n",
      "[iteration 477]-321.336029 1.000911 -0.015274\n",
      "[iteration 478]-323.101105 1.000874 -0.014897\n",
      "[iteration 479]-323.775665 1.000838 -0.014550\n",
      "[iteration 480]-326.063599 1.000812 -0.014291\n",
      "[iteration 481]-381.767822 1.002009 -0.026738\n",
      "[iteration 482]-385.840698 1.001881 -0.025427\n",
      "[iteration 483]-386.423828 1.001786 -0.024488\n",
      "[iteration 484]-387.235443 1.001701 -0.023655\n",
      "[iteration 485]-388.588684 1.001621 -0.022883\n",
      "[iteration 486]-389.094055 1.001544 -0.022128\n",
      "[iteration 487]-390.702728 1.001470 -0.021405\n",
      "[iteration 488]-393.337250 1.001399 -0.020708\n",
      "[iteration 489]-393.038666 1.001332 -0.020025\n",
      "[iteration 490]-393.361725 1.001267 -0.019361\n",
      "[iteration 491]-396.767975 1.001206 -0.018725\n",
      "[iteration 492]-398.951721 1.001147 -0.017989\n",
      "[iteration 493]-399.258636 1.001090 -0.017424\n",
      "[iteration 494]-400.332916 1.001036 -0.016888\n",
      "[iteration 495]-401.805725 1.000984 -0.016368\n",
      "[iteration 496]-403.510986 1.000936 -0.015871\n",
      "[iteration 497]-403.565155 1.000888 -0.015391\n",
      "[iteration 498]-405.555939 1.000844 -0.014931\n",
      "[iteration 499]-406.491791 1.000802 -0.014490\n",
      "[iteration 500]-408.161804 1.000760 -0.014060\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6de65f3d846a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0meager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mrslt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrslt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m           raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m<ipython-input-4-cd5cfd481350>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'layer%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m           raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m<ipython-input-4-cd5cfd481350>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradius\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m             \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'alpha%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m             \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'beta%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    308\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m    309\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Add\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         _ctx._post_execution_callbacks, x, y)\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_get_backward_fn\u001b[0;34m(op_name, attrs, num_inputs, op_inputs, op_outputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_get_backward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0morig_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device(args.device):\n",
    "    for i in range(args.n_iterations):\n",
    "        with eager.GradientTape() as tape:\n",
    "            z = gnn(x, training=True)\n",
    "            rslt, r, b = objective(z, training=True)\n",
    "        gradients = tape.gradient(rslt, gnn.variables)\n",
    "        optimizer.apply_gradients(zip(gradients, gnn.variables)) # TODO tf.train.get_or_create_global_step\n",
    "        \n",
    "        r, b = objective(z)\n",
    "        print('[iteration %d]%f %f %f' % (i + 1, rslt, r, b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
